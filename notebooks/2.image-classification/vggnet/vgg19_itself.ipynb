{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Summary of VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg19_summary(model_path):\n",
    "    net = cv2.dnn.readNetFromONNX(model_path)\n",
    "    if not net.empty():\n",
    "        print(\"Net loaded successfully\")\n",
    "        \n",
    "    # get the types of layers used in the model\n",
    "    print(\"\\nNet contains:\")\n",
    "    for t in net.getLayerTypes():\n",
    "        print(\"\\t{} layers of type {}\".format(net.getLayersCount(t), t))\n",
    "        \n",
    "    layers_ids, in_shapes, out_shapes = net.getLayersShapes([1, 3, 224, 224])\n",
    "    layers_names = net.getLayerNames()\n",
    "    \n",
    "    # get the tensor shapes for the loaded model and specified input shape.\n",
    "    print(\"\\nNet layers shapes: \")\n",
    "    for l in range(len(layers_names)):\n",
    "        in_num, out_num = len(in_shapes[l]), len(out_shapes[l])\n",
    "        print(\"  Layer {} has {} inputs and {} outputs\".format(layers_names[l],\n",
    "                                                             in_num, out_num))\n",
    "        for i in range(in_num):\n",
    "            print(\"\\tinput #{} has shape {}\".format(i, in_shapes[l][i].flatten()))\n",
    "        for i in range(out_num):\n",
    "            print(\"\\toutput #{} has shape {}\".format(i, out_shapes[l][i].flatten()))\n",
    "            \n",
    "    # compute the number of FLOPs\n",
    "    print(\"\\ngflops: \", net.getFLOPS((1, 3, 224, 224)) * 1e-9)\n",
    "    \n",
    "    # report the amount of memory consumed for storing weights and intermediate tensors\n",
    "    w, b = net.getMemoryConsumption((1, 3, 224, 224))\n",
    "    print(\"\\nweights (mb):\", w * 1e-6, \", blobs (mb):\", b * 1e-6)\n",
    "    \n",
    "    # perform a forward pass for a mock input:\n",
    "    blob = cv2.dnn.blobFromImage(np.zeros((224, 224, 3), np.uint8), 1, (224, 224))\n",
    "    net.setInput(blob)\n",
    "    net.forward()\n",
    "    \n",
    "    # report the total time\n",
    "    total, timings = net.getPerfProfile()\n",
    "    tick2ms = 1000 / cv2.getTickFrequency()\n",
    "    print(\"\\ninference (ms): {:2f}\".format(total * tick2ms))\n",
    "    \n",
    "    # report the per layer inference time\n",
    "    layer_names = net.getLayerNames()\n",
    "    print(\"\\n{: <30} {}\".format(\"LAYER\", \"TIME (ms)\"))\n",
    "    for (i,t) in enumerate(timings):\n",
    "        print(\"{: <30} {:.2f}\".format(layer_names[i], t[0] * tick2ms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net loaded successfully\n",
      "\n",
      "Net contains:\n",
      "\t16 layers of type BatchNorm\n",
      "\t16 layers of type Convolution\n",
      "\t2 layers of type Dropout\n",
      "\t3 layers of type Flatten\n",
      "\t3 layers of type InnerProduct\n",
      "\t5 layers of type Pooling\n",
      "\t18 layers of type Relu\n",
      "\t1 layers of type __NetInputLayer__\n",
      "\n",
      "Net layers shapes: \n",
      "  Layer vgg0_conv0_fwd has 1 inputs and 1 outputs\n",
      "\tinput #0 has shape [  1   3 224 224]\n",
      "\toutput #0 has shape [  1   3 224 224]\n",
      "  Layer vgg0_batchnorm0_fwd has 1 inputs and 1 outputs\n",
      "\tinput #0 has shape [  1   3 224 224]\n",
      "\toutput #0 has shape [  1  64 224 224]\n",
      "  Layer vgg0_relu0_fwd has 1 inputs and 1 outputs\n",
      "\tinput #0 has shape [  1  64 224 224]\n",
      "\toutput #0 has shape [  1  64 224 224]\n",
      "  Layer vgg0_conv1_fwd has 1 inputs and 1 outputs\n",
      "\tinput #0 has shape [  1  64 224 224]\n",
      "\toutput #0 has shape [  1  64 224 224]\n",
      "  Layer vgg0_batchnorm1_fwd has 1 inputs and 1 outputs\n",
      "\tinput #0 has shape [  1  64 224 224]\n",
      "\toutput #0 has shape [  1  64 224 224]\n",
      "  Layer vgg0_relu1_fwd has 1 inputs and 1 outputs\n",
      "\tinput #0 has shape [  1  64 224 224]\n",
      "\toutput #0 has shape [  1  64 224 224]\n",
      "  Layer vgg0_pool0_fwd has 1 inputs and 1 outputs\n",
      "\tinput #0 has shape [  1  64 224 224]\n",
      "\toutput #0 has shape [  1  64 224 224]\n",
      "  Layer vgg0_conv2_fwd has 1 inputs and 1 outputs\n",
      "\tinput #0 has shape [  1  64 224 224]\n",
      "\toutput #0 has shape [  1  64 112 112]\n",
      "  Layer vgg0_batchnorm2_fwd has 1 inputs and 1 outputs\n",
      "\tinput #0 has shape [  1  64 112 112]\n",
      "\toutput #0 has shape [  1 128 112 112]\n",
      "  Layer vgg0_relu2_fwd has 1 inputs and 1 outputs\n",
      "\tinput #0 has shape [  1 128 112 112]\n",
      "\toutput #0 has shape [  1 128 112 112]\n",
      "  Layer vgg0_conv3_fwd has 1 inputs and 1 outputs\n",
      "\tinput #0 has shape [  1 128 112 112]\n",
      "\toutput #0 has shape [  1 128 112 112]\n",
      "  Layer vgg0_batchnorm3_fwd has 1 inputs and 1 outputs\n",
      "\tinput #0 has shape [  1 128 112 112]\n",
      "\toutput #0 has shape [  1 128 112 112]\n",
      "  Layer vgg0_relu3_fwd has 1 inputs and 1 outputs\n",
      "\tinput #0 has shape [  1 128 112 112]\n",
      "\toutput #0 has shape [  1 128 112 112]\n",
      "  Layer vgg0_pool1_fwd has 1 inputs and 1 outputs\n",
      "\tinput #0 has shape [  1 128 112 112]\n",
      "\toutput #0 has shape [  1 128 112 112]\n",
      "  Layer vgg0_conv4_fwd has 1 inputs and 1 outputs\n",
      "\tinput #0 has shape [  1 128 112 112]\n",
      "\toutput #0 has shape [  1 128  56  56]\n",
      "  Layer vgg0_batchnorm4_fwd has 1 inputs and 1 outputs\n",
      "\tinput #0 has shape [  1 128  56  56]\n",
      "\toutput #0 has shape [  1 256  56  56]\n",
      "  Layer vgg0_relu4_fwd has 1 inputs and 1 outputs\n",
      "\tinput #0 has shape [  1 256  56  56]\n",
      "\toutput #0 has shape [  1 256  56  56]\n",
      "  Layer vgg0_conv5_fwd has 1 inputs and 1 outputs\n",
      "\tinput #0 has shape [  1 256  56  56]\n",
      "\toutput #0 has shape [  1 256  56  56]\n",
      "  Layer vgg0_batchnorm5_fwd has 1 inputs and 1 outputs\n",
      "\tinput #0 has shape [  1 256  56  56]\n",
      "\toutput #0 has shape [  1 256  56  56]\n",
      "  Layer vgg0_relu5_fwd has 1 inputs and 1 outputs\n",
      "\tinput #0 has shape [  1 256  56  56]\n",
      "\toutput #0 has shape [  1 256  56  56]\n",
      "  Layer vgg0_conv6_fwd has 1 inputs and 1 outputs\n",
      "\tinput #0 has shape [  1 256  56  56]\n",
      "\toutput #0 has shape [  1 256  56  56]\n",
      "  Layer vgg0_batchnorm6_fwd has 1 inputs and 1 outputs\n",
      "\tinput #0 has shape [  1 256  56  56]\n",
      "\toutput #0 has shape [  1 256  56  56]\n",
      "  Layer vgg0_relu6_fwd has 1 inputs and 1 outputs\n",
      "\tinput #0 has shape [  1 256  56  56]\n",
      "\toutput #0 has shape [  1 256  56  56]\n",
      "  Layer vgg0_conv7_fwd has 1 inputs and 1 outputs\n",
      "\tinput #0 has shape [  1 256  56  56]\n",
      "\toutput #0 has shape [  1 256  56  56]\n",
      "  Layer vgg0_batchnorm7_fwd has 1 inputs and 1 outputs\n",
      "\tinput #0 has shape [  1 256  56  56]\n",
      "\toutput #0 has shape [  1 256  56  56]\n",
      "  Layer vgg0_relu7_fwd has 1 inputs and 1 outputs\n",
      "\tinput #0 has shape [  1 256  56  56]\n",
      "\toutput #0 has shape [  1 256  56  56]\n",
      "  Layer vgg0_pool2_fwd has 1 inputs and 1 outputs\n",
      "\tinput #0 has shape [  1 256  56  56]\n",
      "\toutput #0 has shape [  1 256  56  56]\n",
      "  Layer vgg0_conv8_fwd has 1 inputs and 1 outputs\n",
      "\tinput #0 has shape [  1 256  56  56]\n",
      "\toutput #0 has shape [  1 256  28  28]\n",
      "  Layer vgg0_batchnorm8_fwd has 1 inputs and 1 outputs\n",
      "\tinput #0 has shape [  1 256  28  28]\n",
      "\toutput #0 has shape [  1 512  28  28]\n",
      "  Layer vgg0_relu8_fwd has 1 inputs and 1 outputs\n",
      "\tinput #0 has shape [  1 512  28  28]\n",
      "\toutput #0 has shape [  1 512  28  28]\n",
      "  Layer vgg0_conv9_fwd has 1 inputs and 1 outputs\n",
      "\tinput #0 has shape [  1 512  28  28]\n",
      "\toutput #0 has shape [  1 512  28  28]\n",
      "  Layer vgg0_batchnorm9_fwd has 1 inputs and 1 outputs\n",
      "\tinput #0 has shape [  1 512  28  28]\n",
      "\toutput #0 has shape [  1 512  28  28]\n",
      "  Layer vgg0_relu9_fwd has 1 inputs and 1 outputs\n",
      "\tinput #0 has shape [  1 512  28  28]\n",
      "\toutput #0 has shape [  1 512  28  28]\n",
      "  Layer vgg0_conv10_fwd has 1 inputs and 1 outputs\n",
      "\tinput #0 has shape [  1 512  28  28]\n",
      "\toutput #0 has shape [  1 512  28  28]\n",
      "  Layer vgg0_batchnorm10_fwd has 1 inputs and 1 outputs\n",
      "\tinput #0 has shape [  1 512  28  28]\n",
      "\toutput #0 has shape [  1 512  28  28]\n",
      "  Layer vgg0_relu10_fwd has 1 inputs and 1 outputs\n",
      "\tinput #0 has shape [  1 512  28  28]\n",
      "\toutput #0 has shape [  1 512  28  28]\n",
      "  Layer vgg0_conv11_fwd has 1 inputs and 1 outputs\n",
      "\tinput #0 has shape [  1 512  28  28]\n",
      "\toutput #0 has shape [  1 512  28  28]\n",
      "  Layer vgg0_batchnorm11_fwd has 1 inputs and 1 outputs\n",
      "\tinput #0 has shape [  1 512  28  28]\n",
      "\toutput #0 has shape [  1 512  28  28]\n",
      "  Layer vgg0_relu11_fwd has 1 inputs and 1 outputs\n",
      "\tinput #0 has shape [  1 512  28  28]\n",
      "\toutput #0 has shape [  1 512  28  28]\n",
      "  Layer vgg0_pool3_fwd has 1 inputs and 1 outputs\n",
      "\tinput #0 has shape [  1 512  28  28]\n",
      "\toutput #0 has shape [  1 512  28  28]\n",
      "  Layer vgg0_conv12_fwd has 1 inputs and 1 outputs\n",
      "\tinput #0 has shape [  1 512  28  28]\n",
      "\toutput #0 has shape [  1 512  14  14]\n",
      "  Layer vgg0_batchnorm12_fwd has 1 inputs and 1 outputs\n",
      "\tinput #0 has shape [  1 512  14  14]\n",
      "\toutput #0 has shape [  1 512  14  14]\n",
      "  Layer vgg0_relu12_fwd has 1 inputs and 1 outputs\n",
      "\tinput #0 has shape [  1 512  14  14]\n",
      "\toutput #0 has shape [  1 512  14  14]\n",
      "  Layer vgg0_conv13_fwd has 1 inputs and 1 outputs\n",
      "\tinput #0 has shape [  1 512  14  14]\n",
      "\toutput #0 has shape [  1 512  14  14]\n",
      "  Layer vgg0_batchnorm13_fwd has 1 inputs and 1 outputs\n",
      "\tinput #0 has shape [  1 512  14  14]\n",
      "\toutput #0 has shape [  1 512  14  14]\n",
      "  Layer vgg0_relu13_fwd has 1 inputs and 1 outputs\n",
      "\tinput #0 has shape [  1 512  14  14]\n",
      "\toutput #0 has shape [  1 512  14  14]\n",
      "  Layer vgg0_conv14_fwd has 1 inputs and 1 outputs\n",
      "\tinput #0 has shape [  1 512  14  14]\n",
      "\toutput #0 has shape [  1 512  14  14]\n",
      "  Layer vgg0_batchnorm14_fwd has 1 inputs and 1 outputs\n",
      "\tinput #0 has shape [  1 512  14  14]\n",
      "\toutput #0 has shape [  1 512  14  14]\n",
      "  Layer vgg0_relu14_fwd has 1 inputs and 1 outputs\n",
      "\tinput #0 has shape [  1 512  14  14]\n",
      "\toutput #0 has shape [  1 512  14  14]\n",
      "  Layer vgg0_conv15_fwd has 1 inputs and 1 outputs\n",
      "\tinput #0 has shape [  1 512  14  14]\n",
      "\toutput #0 has shape [  1 512  14  14]\n",
      "  Layer vgg0_batchnorm15_fwd has 1 inputs and 1 outputs\n",
      "\tinput #0 has shape [  1 512  14  14]\n",
      "\toutput #0 has shape [  1 512  14  14]\n",
      "  Layer vgg0_relu15_fwd has 1 inputs and 1 outputs\n",
      "\tinput #0 has shape [  1 512  14  14]\n",
      "\toutput #0 has shape [  1 512  14  14]\n",
      "  Layer vgg0_pool4_fwd has 1 inputs and 1 outputs\n",
      "\tinput #0 has shape [  1 512  14  14]\n",
      "\toutput #0 has shape [  1 512  14  14]\n",
      "  Layer flatten_152 has 1 inputs and 1 outputs\n",
      "\tinput #0 has shape [  1 512  14  14]\n",
      "\toutput #0 has shape [  1 512   7   7]\n",
      "  Layer vgg0_dense0_fwd has 1 inputs and 1 outputs\n",
      "\tinput #0 has shape [  1 512   7   7]\n",
      "\toutput #0 has shape [    1 25088]\n",
      "  Layer vgg0_dense0_relu_fwd has 1 inputs and 1 outputs\n",
      "\tinput #0 has shape [    1 25088]\n",
      "\toutput #0 has shape [   1 4096]\n",
      "  Layer vgg0_dropout0_fwd has 1 inputs and 1 outputs\n",
      "\tinput #0 has shape [   1 4096]\n",
      "\toutput #0 has shape [   1 4096]\n",
      "  Layer flatten_157 has 1 inputs and 1 outputs\n",
      "\tinput #0 has shape [   1 4096]\n",
      "\toutput #0 has shape [   1 4096]\n",
      "  Layer vgg0_dense1_fwd has 1 inputs and 1 outputs\n",
      "\tinput #0 has shape [   1 4096]\n",
      "\toutput #0 has shape [   1 4096]\n",
      "  Layer vgg0_dense1_relu_fwd has 1 inputs and 1 outputs\n",
      "\tinput #0 has shape [   1 4096]\n",
      "\toutput #0 has shape [   1 4096]\n",
      "  Layer vgg0_dropout1_fwd has 1 inputs and 1 outputs\n",
      "\tinput #0 has shape [   1 4096]\n",
      "\toutput #0 has shape [   1 4096]\n",
      "  Layer flatten_162 has 1 inputs and 1 outputs\n",
      "\tinput #0 has shape [   1 4096]\n",
      "\toutput #0 has shape [   1 4096]\n",
      "  Layer vgg0_dense2_fwd has 1 inputs and 1 outputs\n",
      "\tinput #0 has shape [   1 4096]\n",
      "\toutput #0 has shape [   1 4096]\n",
      "\n",
      "gflops:  39.468148736\n",
      "\n",
      "weights (mb): 574.757024 , blobs (mb): 185.18416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "inference (ms): 3135.229227\n",
      "\n",
      "LAYER                          TIME (ms)\n",
      "vgg0_conv0_fwd                 62.97\n",
      "vgg0_batchnorm0_fwd            0.00\n",
      "vgg0_relu0_fwd                 0.00\n",
      "vgg0_conv1_fwd                 436.60\n",
      "vgg0_batchnorm1_fwd            0.00\n",
      "vgg0_relu1_fwd                 0.00\n",
      "vgg0_pool0_fwd                 4.81\n",
      "vgg0_conv2_fwd                 207.30\n",
      "vgg0_batchnorm2_fwd            0.00\n",
      "vgg0_relu2_fwd                 0.00\n",
      "vgg0_conv3_fwd                 410.92\n",
      "vgg0_batchnorm3_fwd            0.00\n",
      "vgg0_relu3_fwd                 0.00\n",
      "vgg0_pool1_fwd                 5.35\n",
      "vgg0_conv4_fwd                 166.05\n",
      "vgg0_batchnorm4_fwd            0.00\n",
      "vgg0_relu4_fwd                 0.00\n",
      "vgg0_conv5_fwd                 327.24\n",
      "vgg0_batchnorm5_fwd            0.00\n",
      "vgg0_relu5_fwd                 0.00\n",
      "vgg0_conv6_fwd                 303.94\n",
      "vgg0_batchnorm6_fwd            0.00\n",
      "vgg0_relu6_fwd                 0.00\n",
      "vgg0_conv7_fwd                 311.98\n",
      "vgg0_batchnorm7_fwd            0.00\n",
      "vgg0_relu7_fwd                 0.00\n",
      "vgg0_pool2_fwd                 1.27\n",
      "vgg0_conv8_fwd                 69.20\n",
      "vgg0_batchnorm8_fwd            0.00\n",
      "vgg0_relu8_fwd                 0.00\n",
      "vgg0_conv9_fwd                 183.07\n",
      "vgg0_batchnorm9_fwd            0.00\n",
      "vgg0_relu9_fwd                 0.00\n",
      "vgg0_conv10_fwd                188.88\n",
      "vgg0_batchnorm10_fwd           0.00\n",
      "vgg0_relu10_fwd                0.00\n",
      "vgg0_conv11_fwd                153.23\n",
      "vgg0_batchnorm11_fwd           0.00\n",
      "vgg0_relu11_fwd                0.00\n",
      "vgg0_pool3_fwd                 1.20\n",
      "vgg0_conv12_fwd                42.87\n",
      "vgg0_batchnorm12_fwd           0.00\n",
      "vgg0_relu12_fwd                0.00\n",
      "vgg0_conv13_fwd                65.77\n",
      "vgg0_batchnorm13_fwd           0.00\n",
      "vgg0_relu13_fwd                0.00\n",
      "vgg0_conv14_fwd                51.40\n",
      "vgg0_batchnorm14_fwd           0.00\n",
      "vgg0_relu14_fwd                0.00\n",
      "vgg0_conv15_fwd                50.38\n",
      "vgg0_batchnorm15_fwd           0.00\n",
      "vgg0_relu15_fwd                0.00\n",
      "vgg0_pool4_fwd                 0.62\n",
      "flatten_152                    0.01\n",
      "vgg0_dense0_fwd                74.97\n",
      "vgg0_dense0_relu_fwd           0.00\n",
      "vgg0_dropout0_fwd              0.01\n",
      "flatten_157                    0.00\n",
      "vgg0_dense1_fwd                13.39\n",
      "vgg0_dense1_relu_fwd           0.00\n",
      "vgg0_dropout1_fwd              0.01\n",
      "flatten_162                    0.00\n",
      "vgg0_dense2_fwd                1.77\n"
     ]
    }
   ],
   "source": [
    "model_path = \"./model/vgg19-bn.onnx\"\n",
    "vgg19_summary(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
